# ID3
ID3 is a type of machine learning algorithm that is used by a decision tree to branch nodes. I implemented this algorithm in a machine learning class I took in college. As part of the project, I compared my implementation to scikit-learn's implementation of a decision tree, which worked around 100 times faster than my code. This is unacceptable to me and I immediately started trying to find ways to make my code faster. One of the first things that I learned is that scikit-learn's decision tree doesn't use ID3 as it's decision algorithm and the one it uses is designed to be fast. Unfortunately for me, I had already thought of ways to make my decision tree, so I was morally obligated to make the changes. This repository is the culmination of my work to make my code more efficient.

## Approaches to Efficiency
To find the best ways to increase the speed, I used a line profiler to see what lines of my code were taking the most time to execute. I found two types of improvements to make. I use a lot of numpy arrays and it takes a significant amount of time to create those arrays. Secondly, I use numpy's matrix multiplication function, which takes a significant amount of time to execute.

### Memory
Python was not designed with user memory management in mind, which didn't affect my determination to optimize though this path. Fortunately, many numpy functions take an optional parameter "out" which allows you to give the function a numpy array to store the results instead of it returning a new array. This allows to create arrays only once each time I learn from data instead every time I branch a node. Numpy allows you to resize an existing array without allocating new memory. You do this through numpy's weird slicing, [0:new\_width, 0:new\_height].  Implementing this memory management technique doubled the speed of my code.

### Matrix Multiplication
When I think about processing arrays or matrices, I think that GPU's are a great way to speed up the processing. This is because GPU's are designed to execute the same algorithm on many different pieces data. If I want to make matrix multiplication faster, I should get my GPU do it. I have a Nvidia GTX 1050 on my machine and Nvidia has a very well documented API for using there GPU's which is called CUDA. By default, CUDA is used in C/C++ programs, not python scripts. Thankfully, there is a python library, PyCUDA, that acts as a wrapper for CUDA that I can use to program my GPU to do matrix multiplication. PyCUDA supports using numpy arrays to transfer data between the host and the GPU, which made it much easier to implement the matrix multiplication. By using my GPU, I doubled the speed of my code.

### Both?
After getting such great success in both of my approches to optimization, the next natural step is to use both optimizations together! The first problem I ran into is transfering data between the host and GPU. PyCUDA expects the numpy arrays to be contiguous. Because of the way I was slicing the numpy arrays, it made them not contiguous. I analyzed the size of arrays I was using in my algorithm and I found that less than 1% of of the arrays had a unique size. With that knowledge, I decided to create a list of numpy arrays that were of all the sizes needed for the current learning session. I would keep track of which arrays were being used and when my code needed another numpy array, I would check to see if any arrays of that size were free and if there was I would return the index of that array in the list of arrays. If there were none of that size available, I would create a new array and return the index of that array. When my code didn't need the array any more, it would call a different function with the index of the array that wasn't needed anymore. This would make the array availble to be used at a different part of the algorithm.

This system worked well when I was using small training sizes, when I used 10,000 rows for training, it took about 3 seconds for the algorithm to allocate 10 GB of numpy arrays. At that point, I was out of memory and my computer froze. Fortunatly, my IDE, PyCharm, seems to keep track of how much memory is being used by my code and it becomes unreasonble it sends a SIG\_KILL signal to my program. After a couple of minutes my code was killed and my computer resumed normal behavior. 

The solution to this problem is to keep track of how much memory is being used by the numpy arrays and if it exceeds a certain value, it removes any references to an unused array to encourage the python garbage collector to free that memory. I decided to not implement this solution because I suspect I might run into a problem where I will be constantly creating new arrays because I am deallocating arrays that I need later. Also, the amount of memory management that I would be doing makes me think I should probably continue this project in a different language.  
